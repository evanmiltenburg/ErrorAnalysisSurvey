identifier,comment
1,"I did not find carrying it out particularly challenging as the type of evaluation was ""evaluation by annotation"".  Annotators had already found and categorised the errors, I was only looking for patterns within that, so, for example, within ""Number"" errors, looking at which we ordinal or cardinal, etc."
2,"I found the following challenging when using human evaluation for error analysis:
- Selecting an appropriate error taxonomy for the evaluation. There is no standardised definitions for the error types, and it was hard to know the appropriate level of technical language acceptable for the annotators.
- Achieving a high inter-annotator agreement on a survey."
3,"- establishing the taxonomy of errors
- selecting examples for the analysis
- finding the time for analyzing enough samples (usually operating given a paper deadline)
- handling edge cases (e.g. in fluency errors) or ambiguous cases (error may be of two differnt types)"
4,It was very hard to define or categorise different types of errors.
5,"- it is time consuming
- it is hard to come up with meaningful categories (not too detailed & not too general)
- it may require some subjective judgement "
6,"Categorising the mistakes made by the system, because the errors were often ambiguous between different categories. (Maybe I should have chosen a higher level of abstraction to avoid this, but that would've made the analysis less useful.)"
7,The only complexity is that it adds additional time to prepare the results.
